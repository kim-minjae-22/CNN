{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:16:50.799275Z","iopub.execute_input":"2024-10-23T06:16:50.799598Z","iopub.status.idle":"2024-10-23T06:16:51.805582Z","shell.execute_reply.started":"2024-10-23T06:16:50.799562Z","shell.execute_reply":"2024-10-23T06:16:51.804657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import train and test CSV files\ntrain_csv_loc = \"/kaggle/input/digit-recognizer/train.csv\"\ntest_csv_loc = \"/kaggle/input/digit-recognizer/test.csv\"\ntrain = pd.read_csv(train_csv_loc)\ntest = pd.read_csv(test_csv_loc)\n\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:16:51.807559Z","iopub.execute_input":"2024-10-23T06:16:51.808062Z","iopub.status.idle":"2024-10-23T06:16:57.887589Z","shell.execute_reply.started":"2024-10-23T06:16:51.808018Z","shell.execute_reply":"2024-10-23T06:16:57.886588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the train set so there is also a validation set\ntrain_x, val_x, train_y, val_y = train_test_split(train.iloc[:, 1:], \n                                                  train.iloc[:, 0], \n                                                  test_size=0.2) \n\n# Reset indices so the Dataset can find them with __getitem__ easily\ntrain_x.reset_index(drop=True, inplace=True)\nval_x.reset_index(drop=True, inplace=True)\ntrain_y.reset_index(drop=True, inplace=True)\nval_y.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:16:57.888603Z","iopub.execute_input":"2024-10-23T06:16:57.888864Z","iopub.status.idle":"2024-10-23T06:16:58.6884Z","shell.execute_reply.started":"2024-10-23T06:16:57.888836Z","shell.execute_reply":"2024-10-23T06:16:58.687528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Some more quick data visualization\n# First 10 images of each class in the training set\nfig, ax = plt.subplots(nrows=10, ncols=10, figsize=(10,10))\n\nfor i in range(10): # Column by column\n    num_i = train_x[train_y == i]\n    ax[0][i].set_title(i)\n    for j in range(10): # Row by row\n        ax[j][i].axis('off')\n        ax[j][i].imshow(num_i.iloc[j, :].to_numpy().astype(np.uint8).reshape(28, 28), cmap='gray')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:16:58.690502Z","iopub.execute_input":"2024-10-23T06:16:58.690993Z","iopub.status.idle":"2024-10-23T06:17:02.197954Z","shell.execute_reply.started":"2024-10-23T06:16:58.690954Z","shell.execute_reply":"2024-10-23T06:17:02.197026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MNISTDataSet(Dataset):\n    # images df, labels df, transforms\n    # uses labels to determine if it needs to return X & y or just X in __getitem__\n    def __init__(self, images, labels, transforms=None):\n        self.X = images\n        self.y = labels\n        self.transforms = transforms\n                    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, i):\n        data = self.X.iloc[i, :] # gets the row\n        # reshape the row into the image size \n        # (numpy arrays have the color channels dim last)\n        data = np.array(data).astype(np.uint8).reshape(28, 28, 1) \n        \n        # perform transforms if there are any\n        if self.transforms:\n            data = self.transforms(data)\n        \n        # if !test_set return the label as well, otherwise don't\n        if self.y is not None: # train/val\n            return (data, self.y[i])\n        else: # test\n            return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:25:22.157735Z","iopub.execute_input":"2024-10-23T06:25:22.158114Z","iopub.status.idle":"2024-10-23T06:25:22.166144Z","shell.execute_reply.started":"2024-10-23T06:25:22.158078Z","shell.execute_reply":"2024-10-23T06:25:22.165198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\n\ntransform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])\n\n# transform = transforms.Compose(\n#     [transforms.ToTensor(),\n#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Get datasets using the custom MNIST Dataset for the train, val, and test images\ntrain_dataset = MNISTDataSet(train_x, train_y, transform)\nval_dataset = MNISTDataSet(val_x, val_y, transform)\ntest_dataset = MNISTDataSet(test, None, transform)\n\npartition = {'train': train_dataset, 'val': val_dataset, 'test': test_dataset}\n\nnum_classes = 10 # 0-9","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:25:28.678022Z","iopub.execute_input":"2024-10-23T06:25:28.678409Z","iopub.status.idle":"2024-10-23T06:25:28.685327Z","shell.execute_reply.started":"2024-10-23T06:25:28.67836Z","shell.execute_reply":"2024-10-23T06:25:28.684279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:25:30.931721Z","iopub.execute_input":"2024-10-23T06:25:30.932081Z","iopub.status.idle":"2024-10-23T06:25:30.93841Z","shell.execute_reply.started":"2024-10-23T06:25:30.932047Z","shell.execute_reply":"2024-10-23T06:25:30.937366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_iter = iter(train_loader)\nprint(type(train_iter))\n\nimages, labels = next(train_iter)\n\nprint('images shape on batch size = {}'.format(images.size()))\nprint('labels shape on batch size = {}'.format(labels.size()))\n# images shape on batch size = torch.Size([128, 1, 28, 28])\n# labels shape on batch size = torch.Size([128])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:25:33.042197Z","iopub.execute_input":"2024-10-23T06:25:33.042588Z","iopub.status.idle":"2024-10-23T06:25:33.090506Z","shell.execute_reply.started":"2024-10-23T06:25:33.042543Z","shell.execute_reply":"2024-10-23T06:25:33.089553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# make grid takes tensor as arg\n# tensor : (batchsize, channels, height, width)\ngrid = torchvision.utils.make_grid(images)\n\nplt.imshow(grid.numpy().transpose((1, 2, 0)))\nplt.axis('off')\nplt.title(labels.tolist());","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:19:37.743416Z","iopub.execute_input":"2024-10-23T06:19:37.743787Z","iopub.status.idle":"2024-10-23T06:19:38.05505Z","shell.execute_reply.started":"2024-10-23T06:19:37.743754Z","shell.execute_reply":"2024-10-23T06:19:38.054148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128, 10),\n        )\n                \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        \n        x = x.view(x.size(0), -1)\n        return self.fc(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T07:08:07.1498Z","iopub.execute_input":"2024-10-23T07:08:07.150504Z","iopub.status.idle":"2024-10-23T07:08:07.160805Z","shell.execute_reply.started":"2024-10-23T07:08:07.150457Z","shell.execute_reply":"2024-10-23T07:08:07.159834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dimension_check():\n    net = CNN()\n    x = torch.randn(128, 1, 28, 28)    \n    y = net(x)\n    print(y.size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T07:08:11.814954Z","iopub.execute_input":"2024-10-23T07:08:11.815637Z","iopub.status.idle":"2024-10-23T07:08:11.820227Z","shell.execute_reply.started":"2024-10-23T07:08:11.8156Z","shell.execute_reply":"2024-10-23T07:08:11.819197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dimension_check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T07:08:13.280149Z","iopub.execute_input":"2024-10-23T07:08:13.280497Z","iopub.status.idle":"2024-10-23T07:08:13.335085Z","shell.execute_reply.started":"2024-10-23T07:08:13.280463Z","shell.execute_reply":"2024-10-23T07:08:13.334109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(net, partition, optimizer, criterion, args):\n    trainloader = torch.utils.data.DataLoader(partition['train'], \n                                              batch_size=args.train_batch_size, \n                                              shuffle=True, num_workers=2)\n    net.train()\n\n    correct = 0\n    total = 0\n    train_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n\n        # get the inputs\n        inputs, labels = data\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n        outputs = net(inputs)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    train_loss = train_loss / len(trainloader)\n    train_acc = 100 * correct / total\n    return net, train_loss, train_acc\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T07:08:15.601447Z","iopub.execute_input":"2024-10-23T07:08:15.601812Z","iopub.status.idle":"2024-10-23T07:08:15.610587Z","shell.execute_reply.started":"2024-10-23T07:08:15.601779Z","shell.execute_reply":"2024-10-23T07:08:15.60952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(net, partition, criterion, args):\n    valloader = torch.utils.data.DataLoader(partition['val'], \n                                            batch_size=args.test_batch_size, \n                                            shuffle=False, num_workers=2)\n    net.eval()\n\n    correct = 0\n    total = 0\n    val_loss = 0 \n    with torch.no_grad():\n        for data in valloader:\n            images, labels = data\n            images = images.cuda()\n            labels = labels.cuda()\n            outputs = net(images)\n\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        val_loss = val_loss / len(valloader)\n        val_acc = 100 * correct / total\n    return val_loss, val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:19:48.702628Z","iopub.execute_input":"2024-10-23T06:19:48.703215Z","iopub.status.idle":"2024-10-23T06:19:48.710853Z","shell.execute_reply.started":"2024-10-23T06:19:48.703176Z","shell.execute_reply":"2024-10-23T06:19:48.709815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(net, partition, args):\n    testloader = torch.utils.data.DataLoader(partition['test'], \n                                             batch_size=args.test_batch_size, \n                                             shuffle=False, num_workers=2)\n    net.eval()\n    \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images = images.cuda()\n            labels = labels.cuda()\n\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        test_acc = 100 * correct / total\n    return test_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:19:51.039236Z","iopub.execute_input":"2024-10-23T06:19:51.039998Z","iopub.status.idle":"2024-10-23T06:19:51.046996Z","shell.execute_reply.started":"2024-10-23T06:19:51.03996Z","shell.execute_reply":"2024-10-23T06:19:51.046005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def experiment(partition, args):\n  \n    # net = CNN(model_code = args.model_code,\n    #           in_channels = args.in_channels,\n    #           out_dim = args.out_dim,\n    #           act = args.act,\n    #           use_bn = args.use_bn)\n    net = Net()\n    net.cuda()\n\n    criterion = nn.CrossEntropyLoss()\n    if args.optim == 'SGD':\n        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n    elif args.optim == 'RMSprop':\n        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n    elif args.optim == 'Adam':\n        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n    else:\n        raise ValueError('In-valid optimizer choice')\n    \n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n        \n    for epoch in range(args.epoch):  # loop over the dataset multiple times\n        ts = time.time()\n        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n        val_loss, val_acc = validate(net, partition, criterion, args)\n        te = time.time()\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_accs.append(train_acc)\n        val_accs.append(val_acc)\n        \n        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n        \n    test_acc = test(net, partition, args)    \n    \n    result = {}\n    result['train_losses'] = train_losses\n    result['val_losses'] = val_losses\n    result['train_accs'] = train_accs\n    result['val_accs'] = val_accs\n    result['train_acc'] = train_acc\n    result['val_acc'] = val_acc\n    result['test_acc'] = test_acc\n    return vars(args), result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T07:08:20.998114Z","iopub.execute_input":"2024-10-23T07:08:20.998909Z","iopub.status.idle":"2024-10-23T07:08:21.009555Z","shell.execute_reply.started":"2024-10-23T07:08:20.998869Z","shell.execute_reply":"2024-10-23T07:08:21.008619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import hashlib\nimport json\nfrom os import listdir\nfrom os.path import isfile, join\nimport pandas as pd\n\ndef save_exp_result(setting, result):\n    exp_name = setting['exp_name']\n    del setting['epoch']\n    del setting['test_batch_size']\n\n    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n    result.update(setting)\n    with open(filename, 'w') as f:\n        json.dump(result, f)\n\n    \ndef load_exp_result(exp_name):\n    dir_path = './results'\n    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n    list_result = []\n    for filename in filenames:\n        if exp_name in filename:\n            with open(join(dir_path, filename), 'r') as infile:\n                results = json.load(infile)\n                list_result.append(results)\n    df = pd.DataFrame(list_result) # .drop(columns=[])\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T07:04:31.344147Z","iopub.execute_input":"2024-10-23T07:04:31.344621Z","iopub.status.idle":"2024-10-23T07:04:31.353429Z","shell.execute_reply.started":"2024-10-23T07:04:31.344578Z","shell.execute_reply":"2024-10-23T07:04:31.35244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_acc(var1, var2, df):\n\n    fig, ax = plt.subplots(1, 3)\n    fig.set_size_inches(15, 6)\n    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n\n    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n    \n    ax[0].set_title('Train Accuracy')\n    ax[1].set_title('Validation Accuracy')\n    ax[2].set_title('Test Accuracy')\n\n    \ndef plot_loss_variation(var1, var2, df, **kwargs):\n\n    list_v1 = df[var1].unique()\n    list_v2 = df[var2].unique()\n    list_data = []\n\n    for value1 in list_v1:\n        for value2 in list_v2:\n            row = df.loc[df[var1]==value1]\n            row = row.loc[df[var2]==value2]\n\n            train_losses = list(row.train_losses)[0]\n            val_losses = list(row.val_losses)[0]\n\n            for epoch, train_loss in enumerate(train_losses):\n                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n            for epoch, val_loss in enumerate(val_losses):\n                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n\n    df = pd.DataFrame(list_data)\n    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n    g.add_legend()\n    g.fig.suptitle('Train loss vs Val loss')\n    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n\n\ndef plot_acc_variation(var1, var2, df, **kwargs):\n    list_v1 = df[var1].unique()\n    list_v2 = df[var2].unique()\n    list_data = []\n\n    for value1 in list_v1:\n        for value2 in list_v2:\n            row = df.loc[df[var1]==value1]\n            row = row.loc[df[var2]==value2]\n\n            train_accs = list(row.train_accs)[0]\n            val_accs = list(row.val_accs)[0]\n            test_acc = list(row.test_acc)[0]\n\n            for epoch, train_acc in enumerate(train_accs):\n                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n            for epoch, val_acc in enumerate(val_accs):\n                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n\n    df = pd.DataFrame(list_data)\n    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n\n    def show_acc(x, y, metric, **kwargs):\n        plt.scatter(x, y, alpha=0.3, s=1)\n        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n\n    g.add_legend()\n    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n    plt.subplots_adjust(top=0.89)\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T07:04:35.605578Z","iopub.execute_input":"2024-10-23T07:04:35.605942Z","iopub.status.idle":"2024-10-23T07:04:35.626353Z","shell.execute_reply.started":"2024-10-23T07:04:35.605906Z","shell.execute_reply":"2024-10-23T07:04:35.625274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:20:05.88683Z","iopub.execute_input":"2024-10-23T06:20:05.887202Z","iopub.status.idle":"2024-10-23T06:20:05.893934Z","shell.execute_reply.started":"2024-10-23T06:20:05.887166Z","shell.execute_reply":"2024-10-23T06:20:05.892947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\nfrom copy import deepcopy\nimport torch.optim as optim\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T06:21:52.250414Z","iopub.execute_input":"2024-10-23T06:21:52.251335Z","iopub.status.idle":"2024-10-23T06:21:52.255471Z","shell.execute_reply.started":"2024-10-23T06:21:52.251294Z","shell.execute_reply":"2024-10-23T06:21:52.254541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== Random Seed Initialization ====== #\nseed = 123\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\nparser = argparse.ArgumentParser()\nargs = parser.parse_args(\"\")\nargs.exp_name = \"exp1_lr_model_code\"\n\n# ====== Model ====== #\nargs.model_code = 'VGG11'\nargs.in_channels = 1 #3\nargs.out_dim = 10\nargs.act = 'relu'\n\n# ====== Regularization ======= #\nargs.l2 = 0.00001\nargs.use_bn = True\n\n# ====== Optimizer & Training ====== #\nargs.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\nargs.lr = 0.0015\nargs.epoch = 10\n\nargs.train_batch_size = 256\nargs.test_batch_size = 1024\n\n# ====== Experiment Variable ====== #\nname_var1 = 'lr'\nname_var2 = 'model_code'\nlist_var1 = [0.0001, 0.00001]\nlist_var2 = ['VGG11', 'VGG13']\n\n\nfor var1 in list_var1:\n    for var2 in list_var2:\n        setattr(args, name_var1, var1)\n        setattr(args, name_var2, var2)\n        print(args)\n                \n        setting, result = experiment(partition, deepcopy(args))\n        save_exp_result(setting, result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-23T07:08:28.744239Z","iopub.execute_input":"2024-10-23T07:08:28.744895Z","iopub.status.idle":"2024-10-23T07:08:38.838187Z","shell.execute_reply.started":"2024-10-23T07:08:28.744858Z","shell.execute_reply":"2024-10-23T07:08:38.836585Z"}},"outputs":[],"execution_count":null}]}